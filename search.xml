<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>JetBrains-Clion</title>
      <link href="/joaden/2020/11/26/jetbrains-clion/"/>
      <url>/joaden/2020/11/26/jetbrains-clion/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script><link rel="stylesheet" href="/joaden/css/spoiler.css" type="text/css"><script src="/joaden/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>第四章 pose人体检测容器的测试运行</title>
      <link href="/joaden/2020/10/23/1-3-learn-nano/"/>
      <url>/joaden/2020/10/23/1-3-learn-nano/</url>
      
        <content type="html"><![CDATA[<h1 id="第四章-pose人体检测容器的试运行"><a href="#第四章-pose人体检测容器的试运行" class="headerlink" title="第四章 pose人体检测容器的试运行"></a>第四章 pose人体检测容器的试运行</h1><h3 id="我第一个想做的是人体检测"><a href="#我第一个想做的是人体检测" class="headerlink" title="我第一个想做的是人体检测"></a>我第一个想做的是人体检测</h3><p>    目标检测和跟踪算法是用Python语言实现。使用视频监控截取的测试视频。为了提高计算机视觉、更快的计算速度和对深度网络的干扰，决定使用几个著名的库，如CUDA、OpenCV、dlib和numpy。以下是描述算法运作的步骤:</p><ol><li>加载库和初始化变量;</li><li>负载对象检测模型。我们使用MobileNet模型是因为它的性能;</li><li>启动视频流;</li><li>每帧执行以下操作:</li></ol><p>    每10帧从当前帧构造一个blob，通过网络传递它来获得预测并初始化边界框列表。然后循环检测，如果检测到的对象当前没有被跟踪，初始化跟踪器。<br>OpenCV提供了几个内置的跟踪器;</p><p>(1) 更新追踪;<br>– – (2) 循环跟踪对象，并在其周围画一个矩形;<br>– – (3) 数人;<br>– – (4) 显示额外的信息;</p><ol start="5"><li>停止视频流。</li></ol><p>    考虑了两种计算人数的方法。首先，根据被记录的人数计算出超过一定数量的帧数。其次，根据检查对象的面积。如果面积随着时间的推移而增加，这意味着这个物体离我们的展位或摊位越来越近。我们决定先实现第一种方法。第二种方法将很快实施。</p><p>Jetson Nano vs . Jetson TX2</p><p>    如前所述，我们对Jetson TX2开发套件有一些经验，因此我们决定比较这两个edge AI平台的计算能力。Jetson TX2开发套件具有1.5 TFLOPS的计算性能，几乎是Jetson Nano的三倍。另一方面，它体积大，不便于携带。我们使用上述的目标检测和跟踪算法对这两种设备进行了相同的测试。我们期望TX2的计算速度快三倍。令人惊讶的是，与Jetson TX2相比，Jetson Nano的结果并没有那么糟糕。这说明该算法的操作优化效果非常好。</p><p>    与较大的Jetson TX2相比，Jetson Nano更小，计算性能更差。这是否意味着情况更糟?不客气。对于大多数应用程序，它将提供足够的性能。最后，它取决于应用程序和算法，以更合理地使用设备。</p><h3 id="一、Docker容器内部分析-pose-setup-sh中运行的python-setup-py-代码"><a href="#一、Docker容器内部分析-pose-setup-sh中运行的python-setup-py-代码" class="headerlink" title="一、Docker容器内部分析/pose/setup.sh中运行的python(setup.py)代码"></a>一、Docker容器内部分析/pose/setup.sh中运行的python(setup.py)代码</h3><p>    </p><pre><code>#深度神经网络库import torchimport torch2trtimport trt_pose.models# os 模块提供了非常丰富的方法用来处理文件和目录。import os#模块允许我们启动一个新进程，并连接到它们的输入/输出/错误管道，从而获取返回值。import subprocess#Python中TensorRT库由tensorrt表示。你可以像加载其它任何包一样加载tensorrt。 import tensorrt as trt#模型MODEL = trt_pose.models.densenet121_baseline_att#模型下载网址WEIGHTS = 'https://nvidia.box.com/shared/static/mn7f3a8le9bn8cwihl0v6s9wlm5damaq.pth'#模型保存目录OUTDIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'generated')#模型以新名称保存目录TORCH_WEIGHTS_PATH = os.path.join(OUTDIR, 'densenet121_baseline_att.pth')#学习后的模型保存路径和名称TRT_WEIGHTS_PATH = os.path.join(OUTDIR, 'densenet121_baseline_att_trt.pth')if __name__ == '__main__':    # download weights === #下载并以指定文件名保存    subprocess.call(['wget', WEIGHTS, '-O', TORCH_WEIGHTS_PATH])    # load weights    model = MODEL(18, 42).cuda().eval()    model.load_state_dict(torch.load(TORCH_WEIGHTS_PATH))    # optimize    data = torch.randn((1, 3, 224, 224)).cuda().float()    model_trt = torch2trt.torch2trt(model, [data], fp16_mode=True, max_workspace_size=1 &lt;&lt; 25, log_level=trt.Logger.VERBOSE)    # save    torch.save(model_trt.state_dict(), TRT_WEIGHTS_PATH)</code></pre><p>注：最后因为pose不支持nano而无疾而终、、、</p><h3 id="宿主机-主机-笔记"><a href="#宿主机-主机-笔记" class="headerlink" title="宿主机-主机 笔记"></a>宿主机-主机 笔记</h3><p>1、 subprocess.getstatusoutput(cmd)</p><p>cmd可以直接执行shell命令，而不需要cmd命令以列表输入—-subprocess.getstatusoutput(“cat /proc/meminfo”)<br>返回值包含cmd的执行状态和执行结果，可以直接赋值给某个变量<br>2、subprocess.getoutput(cmd)<br>官方解释：<br>cmd可以直接执行shell命令，而不需要cmd命令以列表输入—subprocess.getoutput(“cat /proc/meminfo”)<br>返回值包含cmd的执行结果，可以直接赋值给某个变量功能和getstatusoutput类似<br>3、subprocess.run(*popenargs, input=None, timeout=None, check=False, **kwargs))</p><p>check等于True的时候，当执行状态不是0时，会抛出CalledProcessError异常提示传入命令参数时，<br>需要以多个命令拆分按照列表形式传入：subprocess.run([‘df’, ‘-h’], stdout=subprocess.PIPE,<br>stderr=subprocess.PIPE,check=True)<br>如果传入参数同时传入shell=True，则传入一个字符串args,shell命令而不是待执行的shell命令序列</p><p>4、subprocess.call()<br>官方解释：传入shell命令参数格式subprocess.check_call([“ls”, “-l”])<br>如果传入参数同时传入shell=True，则可以传入一个字符串shell命令而不是待执行的shell命令列表–subprocess.check_call(“exit 1”, shell=True)返回参数仅返回执行状态码，可通过把结果复制给某个变量查看，如果直接在linux下python编译器执行该命令会直接显示命令执行的结果</p><p>一、model.train()和model.eval()分别在训练和测试中都要写，它们的作用如下：<br>(1)、 model.train()<br>启用BatchNormalization和 Dropout，将BatchNormalization和Dropout置为True<br>(2)、 model.eval()<br>不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False</p><p>二、总结<br>(1)、在训练模块中千万不要忘了写model.train()<br>(2)、在评估（或测试）模块千万不要忘了写model.eval()</p><pre><code>A.在宿主机查看宿主机IP$ ifconfig                          ##假设为xxx.xxx.xxx.xx查看当前显示的环境变量值$ echo $DISPLAY   (要在显示屏查看，其他ssh终端不行)  ##假设为:0或通过socket文件分析：$ ll /tmp/.X11-unix/                            ##假设为X0= ---&gt; :0安装xserver$ sudo apt install x11-xserver-utils$ sudo vim /etc/lightdm/lightdm.conf 增加许可网络连接[SeatDefaults]xserver-allow-tcp=true重启xserver$ sudo systemctl restart lightdm许可所有用户都可访问xserverxhost +B.在docker 容器内# export DISPLAY=xxx.xxx.xxx.xx:0注意：环境变量设置需要每次进docker设置，可以写在：/etc/bash.bashrc 文件中，避免每次进终端时设置</code></pre><p>2.挂载方式<br>挂载方式是在使用image创建docker容器时，通过-v参数设置docker内外路径挂载，使显示xserver设备的socket文件在docker内也可以访问。并通过-e参数设置docker内的DISPLAY参数和宿主机一致。</p><pre><code>A.在宿主机查看宿主机IP$ ifconfig                          ##假设为xxx.xxx.xxx.xx查看当前显示的环境变量值$ echo $DISPLAY   (要在显示屏查看，其他ssh终端不行)  ##假设为:0或通过socket文件分析：$ ll /tmp/.X11-unix/                            ##假设为X0= ---&gt; :0安装xserver$ sudo apt install x11-xserver-utils许可所有用户都可访问xserverxhost +B.在docker 容器创建时-v /tmp/.X11-unix:/tmp/.X11-unix-e DISPLAY=:0例如：docker run -itd --name 容器名 -h 容器主机名 --privileged \           -v /tmp/.X11-unix:/tmp/.X11-unix  \           -e DISPLAY=:0 镜像名或id /bin/bash</code></pre><p>3.验证：<br>使用带有界面功能的时钟软件尝试</p><pre><code>在docker容器中：$ sudo apt-get install xarclock$ xarclock</code></pre><p>应该可以看到xserver端显示器显示时钟界面。</p><h3 id="二、制作jetson-pose容器"><a href="#二、制作jetson-pose容器" class="headerlink" title="二、制作jetson-pose容器"></a>二、制作jetson-pose容器</h3><p>    创建容器名字，端口映射，目录映射，镜像指定</p><h3 id="三、下载jetson-nano教程学习"><a href="#三、下载jetson-nano教程学习" class="headerlink" title="三、下载jetson-nano教程学习"></a>三、下载jetson-nano教程学习</h3><p>    所有的项目在构建过程中，存储jetson-inference库将自动尝试为您下载模型。<br>存储模型的主要站点在Box.com上。但是，来自中国的用户可能无法访问Box.com，因此，<br>如果您的系统无法从其主要位置下载模型，则可以在下面的链接上获取要下载的模型。<br>对于希望在运行时用于推理的每个模型，将下面找到的关联归档文件下载到您的<jetson-inference>/data/networks目录中，然后使用以下命令将其解压缩：</jetson-inference></p><pre><code>cd &lt;jetson-inference&gt;/data/networks/tar -zxvf &lt;model-archive-name&gt;.tar.gz</code></pre><p>事先，像往常一样克隆仓库。此发行页面并不用于存储代码的版本。</p><h4 id="NVCaffe-Python-Workflow"><a href="#NVCaffe-Python-Workflow" class="headerlink" title="===NVCaffe Python Workflow"></a>===NVCaffe Python Workflow</h4><p>TensorRT提供了Python接口，用于加载与优化NVCaffe模型，NVCaffe模型可以执行与存储与PLAN文件中。下列例子展示了实现的工作流程。例子中，你将学习如何在python中使用TensorRT来优化NVCaffe模型。</p><p>Python中TensorRT库由tensorrt表示。</p><p>你可以像加载其它任何包一样加载tensorrt。例如：、</p><p>import tensorrt as trt</p><p>有些工具会经常与python接口TensorRT同时使用，例如PyCUDA，NumPy。PyCUDA处理CUDA操作，比如在GPU上申请内显存，将数据传输到GPU上，结果回传到CPU。NumPy是存储与转移数据的常用工具。</p><p>import pycuda.driver as cuda</p><p>import pycuda.autoinit</p><p>import numpy as np</p><p>本例中，我们导入一个图像处理库（以pillow库为例）与randint。</p><p>from random import randint</p><p>from PIL import Image</p><p>由于我们需要转换NVCaffe模型，我们还需要使用位于tensorrt.parsers的caffeparser。</p><p>from tensorrt.parsers import caffeparser</p><p>通常，第一件事是建立一个日志模块，在模型转换与前向运算时许多地方都会使用。在tensorrt.infer.ConsolueLogger里有个简单的日志模块。</p><p>G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR)</p><pre><code>第二步，定义一些你模型相关的常量。本例中，我们利用DIGITS训练MNIST数据集的模型。</code></pre><p>INPUT_LAYERS = [‘data’]</p><p>OUTPUT_LAYERS = [‘prob’]</p><p>INPUT_H = 28</p><p>INPUT_W = 28</p><p>OUTPUT_SIZE = 10</p><p>此外，定义一些路径。将下列路径映射到例子中你放数据的位置。</p><p>MODEL_PROTOTXT = ‘/data/mnist/mnist.prototxt’</p><p>CAFFE_MODEL = ‘/data/mnist/mnist.caffemodel’</p><p>DATA = ‘/data/mnist/‘</p><p>IMAGE_MEAN = ‘/data/mnist/mnist_mean.binaryproto’</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script><link rel="stylesheet" href="/joaden/css/spoiler.css" type="text/css"><script src="/joaden/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> jetson-nano </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jetson-nano </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第三章 NANO板卡运行容器整理</title>
      <link href="/joaden/2020/10/23/1-2-learn-nano/"/>
      <url>/joaden/2020/10/23/1-2-learn-nano/</url>
      
        <content type="html"><![CDATA[<h1 id="第三章-NANO板卡容器下载和运行"><a href="#第三章-NANO板卡容器下载和运行" class="headerlink" title="第三章 NANO板卡容器下载和运行"></a>第三章 NANO板卡容器下载和运行</h1><p>学习教程： <a href="https://developer.nvidia.com/embedded/learn/tutorials">https://developer.nvidia.com/embedded/learn/tutorials</a><br>NVIDIA资料：  <a href="http://ngc.nvidia.com/">http://ngc.nvidia.com/</a><br>Nvidia jetpack ： <a href="https://developer.nvidia.com/embedded/jetpack-archive">https://developer.nvidia.com/embedded/jetpack-archive</a><br>容器-人体姿态检测： <a href="https://ngc.nvidia.com/catalog/containers/nvidia:jetson-pose">https://ngc.nvidia.com/catalog/containers/nvidia:jetson-pose</a><br>硬件资料文档： <a href="https://docs.nvidia.com/jetson/l4t/index.html">https://docs.nvidia.com/jetson/l4t/index.html</a>   </p><h3 id="一、Docker容器显示图形到宿主机屏幕"><a href="#一、Docker容器显示图形到宿主机屏幕" class="headerlink" title="一、Docker容器显示图形到宿主机屏幕"></a>一、Docker容器显示图形到宿主机屏幕</h3><p> Docker本身的工作模式是命令行的，但是如果运行在docker中的应用需要显示图形界面如何能实现呢？<br>可以通过在宿主机安装xserver，将docker容器视为客户端，通过网络或挂载的方式就可以实现将需要显示的图像显示在宿主机显示器。<br>1.网络方式（此方式也可以用于两主机间）</p><pre><code>A.在宿主机查看宿主机IP$ ifconfig                          ##假设为xxx.xxx.xxx.xx查看当前显示的环境变量值$ echo $DISPLAY   (要在显示屏查看，其他ssh终端不行)  ##假设为:0或通过socket文件分析：$ ll /tmp/.X11-unix/                            ##假设为X0= ---&gt; :0安装xserver$ sudo apt install x11-xserver-utils$ sudo vim /etc/lightdm/lightdm.conf 增加许可网络连接[SeatDefaults]xserver-allow-tcp=true重启xserver$ sudo systemctl restart lightdm许可所有用户都可访问xserverxhost +B.在docker 容器内# export DISPLAY=xxx.xxx.xxx.xx:0注意：环境变量设置需要每次进docker设置，可以写在：/etc/bash.bashrc 文件中，避免每次进终端时设置</code></pre><p>2.挂载方式<br>挂载方式是在使用image创建docker容器时，通过-v参数设置docker内外路径挂载，使显示xserver设备的socket文件在docker内也可以访问。并通过-e参数设置docker内的DISPLAY参数和宿主机一致。</p><pre><code>A.在宿主机查看宿主机IP$ ifconfig                          ##假设为xxx.xxx.xxx.xx查看当前显示的环境变量值$ echo $DISPLAY   (要在显示屏查看，其他ssh终端不行)  ##假设为:0或通过socket文件分析：$ ll /tmp/.X11-unix/                            ##假设为X0= ---&gt; :0安装xserver$ sudo apt install x11-xserver-utils许可所有用户都可访问xserverxhost +B.在docker 容器创建时-v /tmp/.X11-unix:/tmp/.X11-unix-e DISPLAY=:0例如：docker run -itd --name 容器名 -h 容器主机名 --privileged \           -v /tmp/.X11-unix:/tmp/.X11-unix  \           -e DISPLAY=:0 镜像名或id /bin/bash</code></pre><p>3.验证：<br>使用带有界面功能的时钟软件尝试  </p><pre><code>在docker容器中：$ sudo apt-get install xarclock$ xarclock</code></pre><p>应该可以看到xserver端显示器显示时钟界面。</p><h3 id="二、制作运行容器"><a href="#二、制作运行容器" class="headerlink" title="二、制作运行容器"></a>二、制作运行容器</h3><p> 创建容器名字，端口映射，目录映射，镜像指定  </p><ol><li>拉取容器镜像<h1 id="pull-images"><a href="#pull-images" class="headerlink" title="pull images"></a>pull images</h1><code>sudo docker pull XX.com/nvidia-ngc/deepstream-l4t@sha256:XX</code></li><li>制作容器<br>```<br>sudo docker run –name XX_NAME -idt \</li></ol><p>–gpus all <br>–network host <br>-e DISPLAY=$DISPLAY <br>-v /tmp/.X11-unix/:/tmp/.X11-unix <br>-v /home/NAME_XX:/root/workspace/NAME_XX <br>-p 8500-9000:8500-9000 -p 6500-7000:6500-7000 <br>-w /root XX.com/nvidia-ngc/deepstream-l4t@sha256:XX</p><pre><code>3. 运行容器</code></pre><h1 id="enter-docker"><a href="#enter-docker" class="headerlink" title="enter docker"></a>enter docker</h1><p>sudo docker start XX_NAME<br>sudo docker exec -it XX_NAME  /bin/bash</p><pre><code>4. 停止容器和删除`sudo docker stop XX_NAME``sudo docker rm XX_NAME`5. 查看和删除镜像查看所有的镜像`docker images` 5. 查看容器产看所有的容器`docker ps -a`6. 利用容器生成镜像：`docker commit -m "test" -a "test" 6ab59ef4813b ubuntu_test  `7. 为镜像导入包：`root@test:~# cat mysql-5.7.26.tar.gz | docker import - ubuntu_test`8. 导出镜像到本地：`root@test:~# docker save -o ubuntu_test.tar ubuntu_test`9. 导入镜像：`root@test:~# docker load &lt; ubuntu_test.tar`10. 移除镜像：`root@test:~# docker rmi df0bad08f9de---------ID、镜像名称`注：在删除镜像之前要先用 docker rm 删掉依赖于这个镜像的所有容器。11. 导出容器：`root@test:~# docker export 6ab59ef4813b &gt; ubuntu_14.tar`12. 导入容器快照为镜像：`root@test:~# cat ubuntu_14.tar | docker import - test/ubuntu_14:1.0`13. 删除容器：`root@test:~# docker rm 容器名称---------------------- -f参数为删除运行中的容器`14. 删除所有容器：`docker rm $(docker ps -a -q)`### 三、下载jetson-nano教程学习&amp;ensp;所有的项目在构建过程中，存储jetson-inference库将自动尝试为您下载模型。存储模型的主要站点在Box.com上。但是，来自中国的用户可能无法访问Box.com，因此，如果您的系统无法从其主要位置下载模型，则可以在下面的链接上获取要下载的模型。对于希望在运行时用于推理的每个模型，将下面找到的关联归档文件下载到您的&lt;jetson-inference&gt;/data/networks目录中，然后使用以下命令将其解压缩：</code></pre><p>cd <jetson-inference>/data/networks/<br>tar -zxvf <model-archive-name>.tar.gz</model-archive-name></jetson-inference></p><pre><code>事先，像往常一样克隆仓库。此发行页面并不用于存储代码的版本。</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script><link rel="stylesheet" href="/joaden/css/spoiler.css" type="text/css"><script src="/joaden/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> jetson-nano </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jetson-nano </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第二章 NANO开发环境配置</title>
      <link href="/joaden/2020/10/23/1-1-learn-nano/"/>
      <url>/joaden/2020/10/23/1-1-learn-nano/</url>
      
        <content type="html"><![CDATA[<h1 id="第二章-NANO板卡搭建开发环境"><a href="#第二章-NANO板卡搭建开发环境" class="headerlink" title="第二章 NANO板卡搭建开发环境"></a>第二章 NANO板卡搭建开发环境</h1><p>    NVIDIA资料：  <a href="http://ngc.nvidia.com/">http://ngc.nvidia.com/</a></p><h3 id="一、NANO-更新系统"><a href="#一、NANO-更新系统" class="headerlink" title="一、NANO 更新系统"></a>一、NANO 更新系统</h3><p>源替换，加速更新，先拷贝一份，备份，删除所有的原有源，替换成下方的新源</p><pre><code>sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak    #为防止误操作后无法恢复，先备份原文件sources.listsudo gedit /etc/apt/sources.list  </code></pre><p>好用的源，下载速度是原有的百倍。</p><pre><code>deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic main multiverse restricted universedeb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-security main multiverse restricted universedeb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-updates main multiverse restricted universedeb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-backports main multiverse restricted universedeb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic main multiverse restricted universedeb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-security main multiverse restricted universedeb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-updates main multiverse restricted universedeb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-backports main multiverse restricted universe</code></pre><p>更新系统~(根据需要更新，如果因为自身项目的版本限制，最好不要轻易升级)</p><pre><code>sudo apt-get updatesudo apt-get full-upgrade</code></pre><h3 id="四、NANO-安装软件和配置"><a href="#四、NANO-安装软件和配置" class="headerlink" title="四、NANO 安装软件和配置"></a>四、NANO 安装软件和配置</h3><p>    Jetson-nano的OS镜像已经自带了JetPack，cuda，cudnn，opencv等都已经安装好，并有例子，这些例子安装路径如下所示</p><table><thead><tr><th>库名称</th><th align="center">版本</th><th align="left">路径</th></tr></thead><tbody><tr><td>TensorRT</td><td align="center">根据自身情况</td><td align="left">/usr/src/tensorrt/samples/</td></tr><tr><td>CUDA</td><td align="center">根据自身情况</td><td align="left">/usr/local/cuda-/samples/</td></tr><tr><td>cuDNN</td><td align="center">根据自身情况</td><td align="left">/usr/src/cudnn_samples_v7/</td></tr><tr><td>Multimedia API</td><td align="center">根据自身情况</td><td align="left">/usr/src/tegra_multimedia_api/</td></tr><tr><td>VisionWorks</td><td align="center">根据自身情况</td><td align="left">/usr/share/visionworks/sources/samples/ /usr/share/visionworks-tracking/sources/samples/ /usr/share/visionworks-sfm/sources/samples/</td></tr><tr><td>OpenCV</td><td align="center">根据自身情况</td><td align="left">/usr/share/OpenCV/samples/</td></tr></tbody></table><p>（1） 检查CUDA</p><p>Jetson-nano中已经安装了CUDA10.0版本，但是此时你如果运行 nvcc -V是不会成功的，需要你把CUDA的路径写入环境变量中。OS中自带Vim工具 ，所以运行下面的命令编辑环境变量 <code>sudo vim  ~/.bashrc</code><br>在最后添加： 注意自己的版本信息进行调整修改参数</p><pre><code>export CUDA_HOME=/usr/local/cuda-10.0.xxxxxxxexport LD_LIBRARY_PATH=/usr/local/cuda-10.0.xxxxxxxx/lib64:$LD_LIBRARY_PATHexport PATH=/usr/local/cuda-10.0.xxxxx/bin:$PATH</code></pre><p>执行命令<code>source ~/.bashrc</code><br>用命令检查结果 <code>nvcc -V</code></p><p>（2）检查OpenCV<br>Jetson-nano中已经安装了OpenCV3.3版本，可以使用命令检查OpenCV是否安装就绪，如果OpenCv安装就绪，会显示版本号</p><pre><code>pkg-config opencv --modversion</code></pre><p>（3）检查cuDNN<br>Jetson-nano中已经安装好了cuDNN，并有例子可供运行，我们运行一下例子，也正好验证上面的CUDA</p><pre><code>cd /usr/src/cudnn_samples_v7/mnistCUDNN  sudo make     sudo chmod a+x mnistCUDNN # ./mnistCUDNN</code></pre><p>成功则如下显示：</p><pre><code>beckhans@Jetson:/usr/src/cudnn_samples_v7/mnistCUDNN$ ./mnistCUDNN》》》Test passed!</code></pre><h3 id="二、安装python-pip3-jetson-stats-lt-jtop命令-gt"><a href="#二、安装python-pip3-jetson-stats-lt-jtop命令-gt" class="headerlink" title="二、安装python pip3 jetson-stats<jtop命令>"></a>二、安装python pip3 jetson-stats&lt;jtop命令&gt;</h3><ol><li>python pip3<pre><code>sudo apt-get install python3-pip python3-dev#安装pippython -m pip install -U --force-reinstall pip#升级pip至20.1.1</code></pre></li><li>JetSon系列是Nvidia公司推出的面向边缘计算嵌入式平台。<br>在使用nvidia-smi时发现是用不了，搜索之下发现使用jtop代替了<br>补充一下安装jtop的记录<pre><code>pip3 install jetson-stats</code></pre></li></ol><h3 id="三、-安装TensorFlow"><a href="#三、-安装TensorFlow" class="headerlink" title="三、 安装TensorFlow"></a>三、 安装TensorFlow</h3><ol><li>TensorFlow1.13.1的安装</li></ol><p>    C/C++部署有最高运行效率，但开发困难，一般的做法：python做模型的开发调试，最终部署用C/C++部署模型。Nvidia官方给了Tensorflow在Jetson设备上的安装指南，甚至还提供了一个Nvidia编译版本的Tensorflow可以直接安装.</p><ol start="2"><li><p>安装相关的依赖库和工具,系统里面默认是没有python3-pip的，所以需要自己安装。</p><pre><code>sudo apt-get install libhdf5-serial-dev hdf5-tools zlib1g-dev zip libjpeg8-dev libhdf5-dev  python3-pip </code></pre></li><li><p>pip的配置:默认pip是直接从pypi服务器下载这些库的，但是pypi也是在海外的，所以为了下载更快，这里进行一些简单的配置，使用国内pip源，这里使用的是ali的pip源，配置文件如下：</p><pre><code>[global]trusted-host = mirrors.aliyun.comindex-url = http://mirrors.aliyun.com/pypi/simple</code></pre></li><li><p>修改配置文件</p><pre><code>cd ~mkdir .pipnano .pip/pip.conf   # 将上述配置内容写入配置文件即可</code></pre></li><li><p>安装相关的python库,安装过程可能会出现一些错误，如网络错误等，所以需要仔细看一下最终安装的结果，确认安装确实完成了。</p><pre><code>pip3 install -U numpy  # 需要编译安装，用时很长，所以单独安装pip3 install -U h5py # 需要编译安装，用时非常长，我的板子装这个20多分钟才装完pip3 install -U grpcio absl-py py-cpuinfo psutil portpicker grpcio six mock requests gast h5py astor termcolor</code></pre></li></ol><ol start="6"><li>安装TensorFlow<pre><code>pip3 install --pre --extra-index-url \https://developer.download.nvidia.com/compute/redist/jp/v42 tensorflow-gpu</code></pre></li></ol><p>这么安装是直接从网站上下载wheel文件的，也可以用下载工具先去下载好，再用pip执行本地安装。（推荐）<br><a href="https://developer.download.nvidia.com/compute/redist/jp/v44">https://developer.download.nvidia.com/compute/redist/jp/v44</a><br>这个过程同样很漫长，而且还要装一些额外的东西。</p><pre><code>sudo pip install --ignore-installed --upgrade tensorflow_gpu-1.3.0-cp36-cp36m-linux_x86_64.whl#tensorflow-1.15.2+nv20.4-cp36-cp36m-linux_aarch64.whl</code></pre><p>查看命令</p><pre><code>python ==&gt; import tensorflow ==&gt; tensorflow.__version__</code></pre><h3 id="四、opencv4-4安装与问题汇总"><a href="#四、opencv4-4安装与问题汇总" class="headerlink" title="四、opencv4.4安装与问题汇总"></a>四、opencv4.4安装与问题汇总</h3><pre><code>sudo apt-get install cmake #如果已经安装过cmake，则该步骤省略sudo apt-get install build-essential libgtk2.0-dev libgtk-3-dev libavcodec-dev libavformat-dev libjpeg-dev libswscale-dev libtiff5-dev# python3支持sudo apt install python3-dev python3-numpy# streamer支持sudo apt install libgstreamer-plugins-base1.0-dev libgstreamer1.0-dev# 可选的依赖sudo apt install libpng-dev libopenexr-dev libtiff-dev libwebp-devunzip opencv-4.0.0.zip -d .cd opencv-4.0.0/mkdir buildcd build/cmake -D CMAKE_BUILD_TYPE=Release -D OPENCV_GENERATE_PKGCONFIG=YES \-D CMAKE_INSTALL_PREFIX=/usr/local/opencv4 ..# runs 4 jobs in parallel（nano四核）make -j4sudo make installsudo find / -iname opencv4.pc[sudo] password for ywq:/usr/local/opencv4/lib/pkgconfig/opencv4.pcfind: ‘/run/user/1000/gvfs’: Permission denied/home/ywq/Downloads/OpenCV/opencv-4.0.0/build/unix-install/opencv4.pcsudo vim /etc/profile.d/pkgconfig.shexport PKG_CONFIG_PATH=/usr/local/opencv4/lib/pkgconfig:$PKG_CONFIG_PATHsource /etc/profile</code></pre><p>检查：</p><pre><code>pkg-config --libs opencv4 </code></pre><p>结果：<br>-L/usr/local/opencv4/lib -lopencv_ml -lopencv_dnn -lopencv_video <br>-lopencv_stitching -lopencv_objdetect -lopencv_calib3d -lopencv_features2d <br>-lopencv_highgui -lopencv_videoio -lopencv_imgcodecs <br>-lopencv_flann -lopencv_photo -lopencv_gapi -lopencv_imgproc -lopencv_core</p><pre><code>sudo vim /etc/ld.so.conf.d/opencv4.conf    /usr/local/opencv4/libsudo ldconfig</code></pre><h3 id="六、问题汇总"><a href="#六、问题汇总" class="headerlink" title="六、问题汇总"></a>六、问题汇总</h3><ol><li>cmake 编译opencv4参数解析：<br>```</li></ol><p>-D CMAKE_BUILD_TYPE=Release<br>-D CMAKE_INSTALL_PREFIX=/usr/local </p><h1 id="如果要安装opencv-contrib模块，需要指定对应的modules文件夹位置"><a href="#如果要安装opencv-contrib模块，需要指定对应的modules文件夹位置" class="headerlink" title="如果要安装opencv_contrib模块，需要指定对应的modules文件夹位置"></a>如果要安装opencv_contrib模块，需要指定对应的modules文件夹位置</h1><p>-D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib/modules</p><h1 id="基本用不到python2，因此这里只编译python3的opencv版本"><a href="#基本用不到python2，因此这里只编译python3的opencv版本" class="headerlink" title="基本用不到python2，因此这里只编译python3的opencv版本"></a>基本用不到python2，因此这里只编译python3的opencv版本</h1><p>-D PYTHON_DEFAULT_EXECUTABLE=/usr/bin/python3<br>-D BUILD_opencv_python3=ON<br>-D BUILD_opencv_python2=OFF<br>-D PYTHON3_EXCUTABLE=/usr/bin/python3<br>-D PYTHON3_INCLUDE_DIR=</p><h1 id="不需要使用opencv的GPU加速，将CUDA选项关掉"><a href="#不需要使用opencv的GPU加速，将CUDA选项关掉" class="headerlink" title="不需要使用opencv的GPU加速，将CUDA选项关掉"></a>不需要使用opencv的GPU加速，将CUDA选项关掉</h1><p>-D WITH_CUDA=OFF<br>#这个选项非常重要，会生成一个被pkg-config模块使用的opencv4.pc文件，编译opencv的c++项目时可能会用到<br>-D OPENCV_GENERATE_PKGCONFIG=ON </p><pre><code>### 七、vscode 安装wget --content-disposition  \https://packagecloud.io/headmelted/codebuilds/packages/debian/stretch/code-oss_1.32.0-1550644676_arm64.deb/download.deb### 八、google pinyinsudo apt-get install fcitx fcitx-googlepinyin fcitx-module-cloudpinyin fcitx-sunpinyin -ysudo apt-get install fcitx fcitx-googlepinyin -ysudo apt-get -f install### 九、先安装libhdf5-dev、python-h5py，之后再安装keras</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script><link rel="stylesheet" href="/joaden/css/spoiler.css" type="text/css"><script src="/joaden/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> jetson-nano </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jetson-nano </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一章 初始英伟达开发板 jetson-nano</title>
      <link href="/joaden/2020/09/15/1-0-learn-nano/"/>
      <url>/joaden/2020/09/15/1-0-learn-nano/</url>
      
        <content type="html"><![CDATA[<h2 id="第一章-初识NANO板卡"><a href="#第一章-初识NANO板卡" class="headerlink" title="第一章 初识NANO板卡"></a>第一章 初识NANO板卡</h2><p>    NVIDIA 最新推出的Jetson Nano Developer<br>Jetson Nano开发套件是一款功能强大的小型人工智能计算机，插microSD卡即可，可运行多个神经网络，算法计算。</p><h3 id="一、-英伟达Jetson-Nano-是什么"><a href="#一、-英伟达Jetson-Nano-是什么" class="headerlink" title="一、 英伟达Jetson Nano 是什么"></a>一、 英伟达Jetson Nano 是什么</h3><p>    Nvidia Jetson Nano开发工具包是一款功能强大的嵌入式应用AI计算设备。</p><ol><li>使用Nvidia驱动程序完成桌面Linux环境(Ubuntu 18.04);</li><li>库和api，例如:</li></ol><p>-CUDA工具包;<br>-cuDNN - CUDA深度神经网络库;<br>-TensorRT-用于图像分类、分割和目标检测神经网络的深度学习推理运行时;</p><ul><li>VisionWorks -计算机视觉和图像处理软件开发包;</li></ul><ol start="3"><li>多媒体API;</li><li>开发工具- Nsight Eclipse Edition，调试和分析工具;</li><li>文档和示例代码。</li></ol><p>    完全兼容其他流行的机器学习库和框架。<br>    Jetson Nano非常适合作为Edge AI设备，使其成为一种具有无限可能性的设备。</p><h3 id="二、-为什么要用NVIDIA-DIGITS"><a href="#二、-为什么要用NVIDIA-DIGITS" class="headerlink" title="二、 为什么要用NVIDIA DIGITS"></a>二、 为什么要用NVIDIA DIGITS</h3><p>    英伟达通过提供DIGITS (Deep Learning GPU Training System™)，它支持多种任务:</p><ol><li><p>-管理数据集;</p></li><li><p>-设计和训练高精度的深度神经网络用于图像分类、分割、目标检测等任务;</p></li><li><p>-监测模型的性能;</p></li><li><p>-验证和可视化结果;</p></li><li><p>-为部署选择最佳模型。</p><h3 id="三、-我们可以在Jetson-Nano上运行什么样的算法"><a href="#三、-我们可以在Jetson-Nano上运行什么样的算法" class="headerlink" title="三、 我们可以在Jetson Nano上运行什么样的算法?"></a>三、 我们可以在Jetson Nano上运行什么样的算法?</h3><p>    英伟达的Jetson Nano可以运行如下算法:</p></li><li><p>-分类;</p></li><li><p>-目标检测;</p></li><li><p>-对象跟踪;</p></li><li><p>-分割;</p></li><li><p>-姿态和运动估计;</p></li><li><p>-特征跟踪;</p></li><li><p>-视频增强(视频稳定)等。</p></li></ol><p>    这些算法中的许多都可以实时处理，还可以用于许多其他业务和工业应用程序。</p><h3 id="四、英伟达-NANO板卡配置参数"><a href="#四、英伟达-NANO板卡配置参数" class="headerlink" title="四、英伟达 NANO板卡配置参数"></a>四、英伟达 NANO板卡配置参数</h3><p>    开发板上丰富的外围接口，使得开发人员可以轻松连接不同的传感器，以支持各种人工智能应用程序及AI项目的快速搭建。</p><div align="center">![NanoCard](C:\Users\shzto\Pictures\Nvidia\nano.jpg "NanoCard")<div align="left"><ol><li>GPU：NVIDIA MaxwellTM架构，配备128个NVIDIA CUDA核心</li><li>CPU：四核ARM® Cortex®-A57 MP Core处理器</li><li>内存：4GB 64位 LPDDR4</li><li>存储空间：16GB eMMC 5.1闪存</li><li>视频编码：4K @30（H.264/H.265）</li><li>视频解码：4K @60（H.264/H.265）</li><li>摄像头：12通道（3x4或4x2） MIPI CSI-2 DPHY 1.1（1.5Gbps）</li><li>连接：千兆以太网</li><li>显示器：HDMI 2.0或DP1.2 |eDP 1.4|DSI（1x2） 2同步</li><li>UPHY：1x1/2/4 PCIE、1xUSB3.0、3xUSB2.0</li><li>I/O：1xSDIO/2xSPI/6xI2C/2xI2S/GPIO</li><li>尺寸：100 mm x 80 mm x 29 mm </li></ol><h3 id="五、NANO板卡运行"><a href="#五、NANO板卡运行" class="headerlink" title="五、NANO板卡运行"></a>五、NANO板卡运行</h3><p>nano板卡入门指南： <a href="https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro">https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro</a><br>    根据官方提供的SD卡格式化软件和烧录软件，进行格式化和烧录。</p><h5 id="2-1格式化用到SD-Card-Formatter-exe"><a href="#2-1格式化用到SD-Card-Formatter-exe" class="headerlink" title="2.1格式化用到SD Card Formatter.exe"></a>2.1格式化用到SD Card Formatter.exe</h5><div align="center">![SDCardFormatter](C:\Users\shzto\Pictures\Nvidia\SD_Card_Formatter.png "SD_CardFormatter")<div align="left">##### 2.2烧录镜像用到etcher.exe<div align="center">![etcher](C:\Users\shzto\Pictures\Nvidia\etcher.png "etcher")<div align="left"><h3 id="六、-NANO系统启动"><a href="#六、-NANO系统启动" class="headerlink" title="六、 NANO系统启动"></a>六、 NANO系统启动</h3><p>nano插卡，通电启动，完毕。</p></div></div></div></div></div></div><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script><link rel="stylesheet" href="/joaden/css/spoiler.css" type="text/css"><script src="/joaden/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> jetson-nano </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jetson-nano </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
